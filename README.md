# Comparative-analysis-of-learning-process-in-RL-agents

(A) Reinforcement learning (RL) has been a burgeoning field of  Artificial Intelligence in recent years. In this project we investigate the learning process by means of an agent in a controlled self learning environment. (B) A frequent complication in reinforcement learning is the computation required, so it is pivotal to explore ways to accelerate & improve the learning process. As RL models are exceptionally sensitive to hyperparameters, we ask, given an environment and task, which agent performs the best under the given conditions and why, so as to develop a framework that fills the pre-existing lacuna in choosing the right agent for an environment in deep reinforcement learning. (C) We hypothesize that implementing different optimization functions along with a suitable learning rate and the number of time steps would help in the convergence of the model which would ultimately help in maximizing rewards. We also hypothesized that different agents respond disparately to optimizers, so it would be crucial to strike the right balance between the agent and its hyperparameters. (D) To explore this, we prepare a simulation of a robot in a hopper environment, whose task is to reach a target destination as efficiently as possible. Numerous agents viz. DMPO, DDPG, D4PG etc were optimized with pre-defined optimizers namely Gradient Descent, Adam, RMSProp etc. and these reactions were studied. (E) Contrary to our expectations, we observed that there was no unambiguous algorithm that consistently outperformed other agents over a wide range of hyperparameters and reward functions. (F) We conclude that while our study does not report an explicit victor algorithm, it gives valuable insights as to which algorithm may be the best starting point for a particular task. (G) Since our task was specific to maneuvering the movement of robots, it does not generalize well to other dissimilar tasks. Furthermore, it is very onerous to find an optimum trade-off between performance and computational requirements over large time steps, in which case it is advised to settle for slightly substandard performing algorithms to save a lot of computation. Finally, the study can be further extended to include SOTA pre-trained algorithms to understand the pros and cons of transfer learning in deep reinforcement learning.
